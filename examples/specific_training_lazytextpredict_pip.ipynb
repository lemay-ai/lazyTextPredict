{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "specific_training_lazytextpredict_pip.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I26XIgJwTkMj"
      },
      "source": [
        "**Detecting tweets related to disasters**\n",
        "\n",
        "In this example we will use lazytextpredict to determine the best model to detect whether tweets relate to disasters or not. The training data is provided by [Appen](https://appen.com/datasets/combined-disaster-response-data/), and uses english translations of multilingual tweets.\n",
        "\n",
        "To get started we install lazytextpredict..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UWZDlGREo_Wd",
        "outputId": "f543dd23-1641-4566-e877-10565064226e"
      },
      "source": [
        "!pip install lazy-text-predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lazy-text-predict\n",
            "  Downloading https://files.pythonhosted.org/packages/36/21/604d8a0cc9af81be80f98699fb6013a195db9493b1ec1e3fbc264213d9aa/lazy_text_predict-0.0.2-py3-none-any.whl\n",
            "Collecting nlp==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (from lazy-text-predict) (1.7.0+cu101)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.9MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hCollecting transformers==3.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 59.8MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 60.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp==0.4.0->lazy-text-predict) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp==0.4.0->lazy-text-predict) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp==0.4.0->lazy-text-predict) (0.8)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 136kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp==0.4.0->lazy-text-predict) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp==0.4.0->lazy-text-predict) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp==0.4.0->lazy-text-predict) (3.0.12)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101->lazy-text-predict) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101->lazy-text-predict) (3.7.4.3)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1->lazy-text-predict) (20.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 67.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1->lazy-text-predict) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1->lazy-text-predict) (3.12.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->lazy-text-predict) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->lazy-text-predict) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.4.0->lazy-text-predict) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.4.0->lazy-text-predict) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.4.0->lazy-text-predict) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp==0.4.0->lazy-text-predict) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp==0.4.0->lazy-text-predict) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp==0.4.0->lazy-text-predict) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1->lazy-text-predict) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1->lazy-text-predict) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1->lazy-text-predict) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1->lazy-text-predict) (51.1.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f956f91d68fe1df37e228b067617e13264fcf8d294a4307429de61db63937fc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: transformers 3.5.1 has requirement sentencepiece==0.1.91, but you'll have sentencepiece 0.1.95 which is incompatible.\u001b[0m\n",
            "Installing collected packages: xxhash, numpy, pyarrow, nlp, sentencepiece, sacremoses, tokenizers, transformers, threadpoolctl, scikit-learn, lazy-text-predict\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed lazy-text-predict-0.0.2 nlp-0.4.0 numpy-1.18.5 pyarrow-2.0.0 sacremoses-0.0.43 scikit-learn-0.23.2 sentencepiece-0.1.95 threadpoolctl-2.1.0 tokenizers-0.9.3 transformers-3.5.1 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBf59Ps_VNH-"
      },
      "source": [
        "**Download data**\n",
        "\n",
        "Next we have to download the data from the appen website:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhYcn-bXReDt",
        "outputId": "ae89b8f5-25a7-43ba-c3f7-8198b89f6c61"
      },
      "source": [
        "!wget https://datasets.appen.com/appen_datasets/disaster_response_data/disaster_response_messages_training.csv\n",
        "!wget https://datasets.appen.com/appen_datasets/disaster_response_data/disaster_response_messages_test.csv\n",
        "!wget https://datasets.appen.com/appen_datasets/disaster_response_data/disaster_response_messages_validation.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-16 14:22:46--  https://datasets.appen.com/appen_datasets/disaster_response_data/disaster_response_messages_training.csv\n",
            "Resolving datasets.appen.com (datasets.appen.com)... 54.210.240.39, 54.210.154.205, 54.86.241.79\n",
            "Connecting to datasets.appen.com (datasets.appen.com)|54.210.240.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5746561 (5.5M) [text/csv]\n",
            "Saving to: ‘disaster_response_messages_training.csv’\n",
            "\n",
            "disaster_response_m 100%[===================>]   5.48M  10.6MB/s    in 0.5s    \n",
            "\n",
            "2021-01-16 14:22:47 (10.6 MB/s) - ‘disaster_response_messages_training.csv’ saved [5746561/5746561]\n",
            "\n",
            "--2021-01-16 14:22:47--  https://datasets.appen.com/appen_datasets/disaster_response_data/disaster_response_messages_test.csv\n",
            "Resolving datasets.appen.com (datasets.appen.com)... 54.210.240.39, 54.210.154.205, 54.86.241.79\n",
            "Connecting to datasets.appen.com (datasets.appen.com)|54.210.240.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 715427 (699K) [text/csv]\n",
            "Saving to: ‘disaster_response_messages_test.csv’\n",
            "\n",
            "disaster_response_m 100%[===================>] 698.66K  1.77MB/s    in 0.4s    \n",
            "\n",
            "2021-01-16 14:22:48 (1.77 MB/s) - ‘disaster_response_messages_test.csv’ saved [715427/715427]\n",
            "\n",
            "--2021-01-16 14:22:48--  https://datasets.appen.com/appen_datasets/disaster_response_data/disaster_response_messages_validation.csv\n",
            "Resolving datasets.appen.com (datasets.appen.com)... 54.210.240.39, 54.210.154.205, 54.86.241.79\n",
            "Connecting to datasets.appen.com (datasets.appen.com)|54.210.240.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 739819 (722K) [text/csv]\n",
            "Saving to: ‘disaster_response_messages_validation.csv’\n",
            "\n",
            "disaster_response_m 100%[===================>] 722.48K  2.13MB/s    in 0.3s    \n",
            "\n",
            "2021-01-16 14:22:49 (2.13 MB/s) - ‘disaster_response_messages_validation.csv’ saved [739819/739819]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiUCaDpTLOc"
      },
      "source": [
        "**Data handling**\n",
        "\n",
        "Now that we have the data we need to read the downloaded data into pandas dataframes. These dataframes are concatenated into one big dataset, which will be split into train and test sets by lazytextpredict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "SwHotvv9R3Fd",
        "outputId": "098c6e2e-1d9e-4237-90b9-95407a899399"
      },
      "source": [
        "import pandas as pd\n",
        "training_data=pd.read_csv(filepath_or_buffer='/content/disaster_response_messages_training.csv')\n",
        "test_data=pd.read_csv(filepath_or_buffer='/content/disaster_response_messages_test.csv')\n",
        "validation_data=pd.read_csv(filepath_or_buffer='/content/disaster_response_messages_validation.csv')\n",
        "data=training_data.append(test_data)\n",
        "data=data.append(validation_data) ## put all the data into one df so that test_train_split can stratify it properly \n",
        "##(I included the verification data because there is no method included in lazytextpredict at the moment)\n",
        "data = data[data.related != 2]\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>split</th>\n",
              "      <th>message</th>\n",
              "      <th>original</th>\n",
              "      <th>genre</th>\n",
              "      <th>related</th>\n",
              "      <th>PII</th>\n",
              "      <th>request</th>\n",
              "      <th>offer</th>\n",
              "      <th>aid_related</th>\n",
              "      <th>medical_help</th>\n",
              "      <th>medical_products</th>\n",
              "      <th>search_and_rescue</th>\n",
              "      <th>security</th>\n",
              "      <th>military</th>\n",
              "      <th>child_alone</th>\n",
              "      <th>water</th>\n",
              "      <th>food</th>\n",
              "      <th>shelter</th>\n",
              "      <th>clothing</th>\n",
              "      <th>money</th>\n",
              "      <th>missing_people</th>\n",
              "      <th>refugees</th>\n",
              "      <th>death</th>\n",
              "      <th>other_aid</th>\n",
              "      <th>infrastructure_related</th>\n",
              "      <th>transport</th>\n",
              "      <th>buildings</th>\n",
              "      <th>electricity</th>\n",
              "      <th>tools</th>\n",
              "      <th>hospitals</th>\n",
              "      <th>shops</th>\n",
              "      <th>aid_centers</th>\n",
              "      <th>other_infrastructure</th>\n",
              "      <th>weather_related</th>\n",
              "      <th>floods</th>\n",
              "      <th>storm</th>\n",
              "      <th>fire</th>\n",
              "      <th>earthquake</th>\n",
              "      <th>cold</th>\n",
              "      <th>other_weather</th>\n",
              "      <th>direct_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>train</td>\n",
              "      <td>Weather update - a cold front from Cuba that c...</td>\n",
              "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
              "      <td>direct</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>train</td>\n",
              "      <td>Is the Hurricane over or is it not over</td>\n",
              "      <td>Cyclone nan fini osinon li pa fini</td>\n",
              "      <td>direct</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>train</td>\n",
              "      <td>says: west side of Haiti, rest of the country ...</td>\n",
              "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
              "      <td>direct</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14</td>\n",
              "      <td>train</td>\n",
              "      <td>Information about the National Palace-</td>\n",
              "      <td>Informtion au nivaux palais nationl</td>\n",
              "      <td>direct</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>train</td>\n",
              "      <td>Storm at sacred heart of jesus</td>\n",
              "      <td>Cyclone Coeur sacr de jesus</td>\n",
              "      <td>direct</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  split  ... other_weather direct_report\n",
              "0   2  train  ...             0             0\n",
              "1   7  train  ...             0             0\n",
              "2  12  train  ...             0             0\n",
              "3  14  train  ...             0             0\n",
              "4  15  train  ...             0             0\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJQwhfxXV2ec"
      },
      "source": [
        "**Initial training**\n",
        "\n",
        "Now that we have the data we can put it into lazytextpredict. \n",
        "\n",
        "We do this by creating an instance of the LTP class with \n",
        "\n",
        "```Xdata=data['message'] ``` i.e. the translated tweet text\n",
        "\n",
        "and \n",
        "\n",
        "``` Ydata=data['related'] ``` i.e. if the message is related to a disaster.\n",
        "\n",
        "We train all the models with a small number of training epochs (5) to get an idea of how effective our models are with this data. This should take about 20 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63f79839b471456eaa48e3a6423548bc",
            "24bef221b83445ff9fc0a635bf9a33c7",
            "e29b588a84814f198ed68c415a54d754",
            "e65b6647188340b9a9da5c7d1d6340ce",
            "1c4d5d15f668485f90ca68555f6d3dcc",
            "27c6688e68a0471d879c2ea09359ecba",
            "d9648f43f02e4abba886641c9e2632e6",
            "05b2291d6db444aca98a1385d8396346",
            "239a92c31fdf4feda74eb8cd3dab87b2",
            "9bca45d576c44de4bedd55419ec79c76",
            "abe086d316a845979b6961df8aefaac5",
            "1272a3cfd28a4f418206cf4bf4a125a5",
            "37b9e702a25540ab83c756c4049f4b1e",
            "6a6c3a53f95042589ea620bc2815d8c9",
            "57e8252998444454abe320f504160680",
            "3ed88519e5934caf9b2a387b62eb7bed",
            "77043b0bea5d4c15a8f35bb3084283f4"
          ]
        },
        "id": "4bNdmLGRUQ33",
        "outputId": "a91beacc-7ead-46c1-9ff8-55469e9cdd34"
      },
      "source": [
        "from lazytextpredict import basic_classification\n",
        "trial=basic_classification.LTP(Xdata=data['message'],Ydata=data['related'])\n",
        "trial.run(training_epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converting pandas series to list\n",
            "converting pandas series to list\n",
            "X_train length: 1302\n",
            "X_test length: 1303\n",
            "Y_train length: 1302\n",
            "Y_test length: 1303\n",
            "Training on a dataset with 2 labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63f79839b471456eaa48e3a6423548bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24bef221b83445ff9fc0a635bf9a33c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e29b588a84814f198ed68c415a54d754",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e65b6647188340b9a9da5c7d1d6340ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c4d5d15f668485f90ca68555f6d3dcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27c6688e68a0471d879c2ea09359ecba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return function(data_struct)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [410/410 06:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:21]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Trainer is attempting to log a value of \"{'0': {'precision': 0.7843137254901961, 'recall': 0.39215686274509803, 'f1-score': 0.522875816993464, 'support': 306}, '1': {'precision': 0.8382608695652174, 'recall': 0.966900702106319, 'f1-score': 0.8979972054028877, 'support': 997}, 'accuracy': 0.8319263238679969, 'macro avg': {'precision': 0.8112872975277068, 'recall': 0.6795287824257085, 'f1-score': 0.7104365111981759, 'support': 1303}, 'weighted avg': {'precision': 0.8255917781707763, 'recall': 0.8319263238679969, 'f1-score': 0.8099026966896999, 'support': 1303}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8546987771987915, 'eval_accuracy': 0.8319263238679969, 'eval_f1': 0.8099026966896999, 'eval_precision': 0.8255917781707763, 'eval_recall': 0.8319263238679969, 'eval_full_report': {'0': {'precision': 0.7843137254901961, 'recall': 0.39215686274509803, 'f1-score': 0.522875816993464, 'support': 306}, '1': {'precision': 0.8382608695652174, 'recall': 0.966900702106319, 'f1-score': 0.8979972054028877, 'support': 997}, 'accuracy': 0.8319263238679969, 'macro avg': {'precision': 0.8112872975277068, 'recall': 0.6795287824257085, 'f1-score': 0.7104365111981759, 'support': 1303}, 'weighted avg': {'precision': 0.8255917781707763, 'recall': 0.8319263238679969, 'f1-score': 0.8099026966896999, 'support': 1303}}, 'epoch': 5.0}\n",
            "Training on a dataset with 2 labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9648f43f02e4abba886641c9e2632e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05b2291d6db444aca98a1385d8396346",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "239a92c31fdf4feda74eb8cd3dab87b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=47376696.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bca45d576c44de4bedd55419ec79c76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abe086d316a845979b6961df8aefaac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [410/410 06:15, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Trainer is attempting to log a value of \"{'0': {'precision': 0.49236641221374045, 'recall': 0.4215686274509804, 'f1-score': 0.4542253521126761, 'support': 306}, '1': {'precision': 0.829971181556196, 'recall': 0.8665997993981945, 'f1-score': 0.8478900883218843, 'support': 997}, 'accuracy': 0.7620874904067536, 'macro avg': {'precision': 0.6611687968849682, 'recall': 0.6440842134245874, 'f1-score': 0.6510577202172803, 'support': 1303}, 'weighted avg': {'precision': 0.7506871758625725, 'recall': 0.7620874904067536, 'f1-score': 0.7554408102865675, 'support': 1303}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4899747669696808, 'eval_accuracy': 0.7620874904067536, 'eval_f1': 0.7554408102865675, 'eval_precision': 0.7506871758625725, 'eval_recall': 0.7620874904067536, 'eval_full_report': {'0': {'precision': 0.49236641221374045, 'recall': 0.4215686274509804, 'f1-score': 0.4542253521126761, 'support': 306}, '1': {'precision': 0.829971181556196, 'recall': 0.8665997993981945, 'f1-score': 0.8478900883218843, 'support': 997}, 'accuracy': 0.7620874904067536, 'macro avg': {'precision': 0.6611687968849682, 'recall': 0.6440842134245874, 'f1-score': 0.6510577202172803, 'support': 1303}, 'weighted avg': {'precision': 0.7506871758625725, 'recall': 0.7620874904067536, 'f1-score': 0.7554408102865675, 'support': 1303}}, 'epoch': 5.0}\n",
            "Training on a dataset with 2 labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1272a3cfd28a4f418206cf4bf4a125a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37b9e702a25540ab83c756c4049f4b1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a6c3a53f95042589ea620bc2815d8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57e8252998444454abe320f504160680",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ed88519e5934caf9b2a387b62eb7bed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77043b0bea5d4c15a8f35bb3084283f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [410/410 06:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 00:21]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Trainer is attempting to log a value of \"{'0': {'precision': 0.7412935323383084, 'recall': 0.4869281045751634, 'f1-score': 0.5877712031558185, 'support': 306}, '1': {'precision': 0.8575317604355717, 'recall': 0.9478435305917753, 'f1-score': 0.9004287756074322, 'support': 997}, 'accuracy': 0.8396009209516501, 'macro avg': {'precision': 0.79941264638694, 'recall': 0.7173858175834693, 'f1-score': 0.7440999893816254, 'support': 1303}, 'weighted avg': {'precision': 0.8302340645048254, 'recall': 0.8396009209516501, 'f1-score': 0.8270034362596244, 'support': 1303}}\" of type <class 'dict'> for key \"eval/full_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.574372410774231, 'eval_accuracy': 0.8396009209516501, 'eval_f1': 0.8270034362596244, 'eval_precision': 0.8302340645048254, 'eval_recall': 0.8396009209516501, 'eval_full_report': {'0': {'precision': 0.7412935323383084, 'recall': 0.4869281045751634, 'f1-score': 0.5877712031558185, 'support': 306}, '1': {'precision': 0.8575317604355717, 'recall': 0.9478435305917753, 'f1-score': 0.9004287756074322, 'support': 997}, 'accuracy': 0.8396009209516501, 'macro avg': {'precision': 0.79941264638694, 'recall': 0.7173858175834693, 'f1-score': 0.7440999893816254, 'support': 1303}, 'weighted avg': {'precision': 0.8302340645048254, 'recall': 0.8396009209516501, 'f1-score': 0.8270034362596244, 'support': 1303}}, 'epoch': 5.0}\n",
            "Training on a dataset with 2 labels\n",
            "ERROR\n",
            "best parameters are:\n",
            "{'clf__alpha': 0.001, 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "{'eval_loss': 0.21105141980046047, 'eval_accuracy': 0.7889485801995395, 'eval_f1': 0.5678588011119352, 'eval_precision': 0.7650304461289922, 'eval_recall': 0.569905795818829, 'eval_full_report': '              precision    recall  f1-score   support\\n\\n           0       0.74      0.16      0.26       306\\n           1       0.79      0.98      0.88       997\\n\\n    accuracy                           0.79      1303\\n   macro avg       0.77      0.57      0.57      1303\\nweighted avg       0.78      0.79      0.73      1303\\n'}\n",
            "Training on a dataset with 2 labels\n",
            "ERROR\n",
            "best parameters are:\n",
            "{'clf__alpha': 1, 'clf__fit_prior': False, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "{'eval_loss': 0.21642363775901766, 'eval_accuracy': 0.7835763622409824, 'eval_f1': 0.5455550608489166, 'eval_precision': 0.7542256027043857, 'eval_recall': 0.5562029224929691, 'eval_full_report': '              precision    recall  f1-score   support\\n\\n           0       0.72      0.13      0.22       306\\n           1       0.79      0.98      0.87       997\\n\\n    accuracy                           0.78      1303\\n   macro avg       0.75      0.56      0.55      1303\\nweighted avg       0.77      0.78      0.72      1303\\n'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuIiJAKXkqn"
      },
      "source": [
        "**Analyze results**\n",
        "\n",
        "Let's compare the models using the print_metrics_table method, and a couple of dummy tweets. We can see that roberta-base performs a little better than all the other models on this challenge, so we'll use that going forward with focused training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLdZGzrAqiQJ",
        "outputId": "0f4aeb82-f893-4fad-aab4-2cbc7dd9bc84"
      },
      "source": [
        "trial.print_metrics_table()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Model            loss        accuracy              f1       precision          recall\n",
            "        bert-base-uncased          0.8547         0.83193          0.8099         0.82559         0.83193\n",
            "           albert-base-v2         0.48997         0.76209         0.75544         0.75069         0.76209\n",
            "             roberta-base         0.57437          0.8396           0.827         0.83023          0.8396\n",
            "               linear_SVM         0.21105         0.78895         0.56786         0.76503         0.56991\n",
            "multinomial_naive_bayesian         0.21642         0.78358         0.54556         0.75423          0.5562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFDYJxgKqkyp",
        "outputId": "c18ef6b5-674e-4672-f62a-dfb14e003481"
      },
      "source": [
        "trial.predict(text=\"Cardi B's new album is the best\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n",
            "{'label': 'LABEL_0', 'score': 0.9794496297836304}\n",
            "albert-base-v2\n",
            "{'label': 'LABEL_1', 'score': 0.5572738647460938}\n",
            "roberta-base\n",
            "{'label': 'LABEL_0', 'score': 0.690045952796936}\n",
            "linear_SVM\n",
            "[1]\n",
            "multinomial_naive_bayesian\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jPWRzz-ZNtM",
        "outputId": "0b349853-0253-46c5-8cd6-77447b1f1590"
      },
      "source": [
        "trial.predict(text=\"All good now, just needed some R&R\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n",
            "{'label': 'LABEL_0', 'score': 0.9884157776832581}\n",
            "albert-base-v2\n",
            "{'label': 'LABEL_0', 'score': 0.8769664168357849}\n",
            "roberta-base\n",
            "{'label': 'LABEL_1', 'score': 0.9080235362052917}\n",
            "linear_SVM\n",
            "[1]\n",
            "multinomial_naive_bayesian\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESQy2MuPZuG7",
        "outputId": "06b4349d-69e9-4502-c7ef-0e3c3e2260ea"
      },
      "source": [
        "trial.predict(text=\"this rally is getting out of hand\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n",
            "{'label': 'LABEL_1', 'score': 0.9860490560531616}\n",
            "albert-base-v2\n",
            "{'label': 'LABEL_0', 'score': 0.7396048307418823}\n",
            "roberta-base\n",
            "{'label': 'LABEL_1', 'score': 0.8055053353309631}\n",
            "linear_SVM\n",
            "[1]\n",
            "multinomial_naive_bayesian\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO7sWeDnZWXE",
        "outputId": "45e448b1-c1bb-47bf-8f63-d0160f05d135"
      },
      "source": [
        "trial.predict(text=\"SOS, flood warning in Gatineau. Too much gravy on our poutine!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n",
            "{'label': 'LABEL_1', 'score': 0.9996545314788818}\n",
            "albert-base-v2\n",
            "{'label': 'LABEL_1', 'score': 0.9297879934310913}\n",
            "roberta-base\n",
            "{'label': 'LABEL_1', 'score': 0.9968279600143433}\n",
            "linear_SVM\n",
            "[1]\n",
            "multinomial_naive_bayesian\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcmx3fCIlRtx"
      },
      "source": [
        "**Focused training**\n",
        "\n",
        "Now that we have identified the model we want to use we can train and test it specifically..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "6aoMpY3wqlJL",
        "outputId": "4b61c570-4b51-403b-b952-48db4305e40c"
      },
      "source": [
        "focused_trial=basic_classification.LTP(Xdata=data['message'],Ydata=data['related'],test_frac=0.1,train_frac=0.9)\r\n",
        "focused_trial.run(focused=True,focused_model='roberta-base',training_epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-60149ee09160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfocused_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbasic_classification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLTP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'related'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfocused_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfocused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfocused_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'Xdata'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otiCVzrojnjI",
        "outputId": "bf65d5df-8f9f-4412-dc1d-edd424069fb0"
      },
      "source": [
        "focused_trial.print_metrics_table()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Model            loss        accuracy              f1       precision          recall\n",
            "             roberta-base         0.61509         0.84539         0.84042         0.83897         0.84539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWz8GIFJqtNm",
        "outputId": "a806a014-66db-4f08-8575-112922689fba"
      },
      "source": [
        "trial.predict(model_name='roberta-base', focused=True,text='This movie was really something. I loved every second of it')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta-base\n",
            "{'label': 'LABEL_0', 'score': 0.990229070186615}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyrcksbIjN4l",
        "outputId": "8aa34359-3ca2-46f2-dbba-9cb78956985f"
      },
      "source": [
        "trial.predict(model_name='roberta-base', focused=True,text='Oh no, trouble is brewing')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta-base\n",
            "{'label': 'LABEL_1', 'score': 0.9322614073753357}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlv4fA8VjR3C",
        "outputId": "1ac7ecf1-ebae-49ef-a8d6-8994b90b4304"
      },
      "source": [
        "trial.predict(model_name='roberta-base', focused=True,text='Get out of town! A maple syrup disaster!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta-base\n",
            "{'label': 'LABEL_1', 'score': 0.9910778403282166}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbVmg8URjYWQ",
        "outputId": "11de99ed-2cb0-4fbc-f266-40a7edebf029"
      },
      "source": [
        "trial.predict(model_name='roberta-base', focused=True,text='Watch out, jack frost is about')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta-base\n",
            "{'label': 'LABEL_1', 'score': 0.9950404763221741}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCFhNQHtjejI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}